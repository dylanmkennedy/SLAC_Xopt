{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 2D Injector Optimization with NN Prior at LCLS\n",
    "Aiming to optimize transverse beam size in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-17T02:42:59.049600800Z",
     "start_time": "2023-08-17T02:42:59.040600400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# optionally add scripts location to path\n",
    "if True:\n",
    "    import sys\n",
    "    sys.path.append(\"../../\")\n",
    "    sys.path.append(\"../\")\n",
    "\n",
    "run_dir = \"/home/physics3/ml_tuning/20231002_LCLS_Injector/\"\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Set up image diagnostic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-17T02:43:02.093712200Z",
     "start_time": "2023-08-17T02:42:59.051600200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from scripts.image import ImageDiagnostic\n",
    "import yaml\n",
    "\n",
    "fname = \"../OTR3_config.yml\"\n",
    "image_diagnostic = ImageDiagnostic.parse_obj(yaml.safe_load(open(fname)))\n",
    "image_diagnostic.save_image_location = run_dir\n",
    "image_diagnostic.n_fitting_restarts = 2\n",
    "image_diagnostic.visualize = False\n",
    "image_diagnostic.background_file = run_dir + \"OTRS_IN20_621_background.npy\"\n",
    "print(image_diagnostic.yaml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_diagnostic.measure_background(file_location=run_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_diagnostic.background_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(image_diagnostic.background_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_diagnostic.test_measurement()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define VOCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from xopt import VOCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../variables.csv\"\n",
    "VARIABLE_RANGES = pd.read_csv(filename, index_col=0, header=None).T.to_dict(orient='list')\n",
    "\n",
    "IMAGE_CONSTRAINTS = {\n",
    "    \"bb_penalty\": [\"LESS_THAN\", 0.0],\n",
    "    \"log10_total_intensity\": [\"GREATER_THAN\", image_diagnostic.min_log_intensity]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIABLES = [\"SOLN:IN20:121:BCTRL\", \"QUAD:IN20:121:BCTRL\", \"QUAD:IN20:122:BCTRL\",\n",
    "             \"QUAD:IN20:361:BCTRL\", \"QUAD:IN20:371:BCTRL\", \"QUAD:IN20:425:BCTRL\",\n",
    "             \"QUAD:IN20:441:BCTRL\"]  # , \"QUAD:IN20:511:BCTRL\", \"QUAD:IN20:525:BCTRL\"\n",
    "\n",
    "vocs = VOCS(\n",
    "    variables = {ele: VARIABLE_RANGES[ele] for ele in VARIABLES},\n",
    "    constraints = IMAGE_CONSTRAINTS,\n",
    "    objectives = {\"total_size\": \"MINIMIZE\"},\n",
    ")\n",
    "print(vocs.as_yaml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define evaluate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from epics import caput, caget_many"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_predictions(input_dict, generator = None):\n",
    "    output_dict = {}\n",
    "    for output_name in generator.vocs.output_names:\n",
    "        if generator is not None and generator.data is not None and not generator.data.empty:\n",
    "            gp = generator.model.models[generator.vocs.output_names.index(output_name)]\n",
    "            x = torch.tensor([input_dict[k] for k in generator.vocs.variable_names], dtype=torch.double).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                _x = gp.input_transform.transform(x)\n",
    "                _x = gp.mean_module(_x)\n",
    "                prior_mean = gp.outcome_transform.untransform(_x)[0].item()\n",
    "                posterior = gp.posterior(x)\n",
    "                posterior_mean = posterior.mean.item()\n",
    "                posterior_sd = torch.sqrt(posterior.mvn.variance).item()\n",
    "        else:\n",
    "            prior_mean, posterior_mean, posterior_sd = np.nan, np.nan, np.nan\n",
    "    \n",
    "        output_dict[output_name + \"_prior_mean\"] = prior_mean\n",
    "        output_dict[output_name + \"_posterior_mean\"] = posterior_mean\n",
    "        output_dict[output_name + \"_posterior_sd\"] = posterior_sd\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_beamsize(input_dict, generator = None):\n",
    "    global image_diagnostic\n",
    "    # set PVs\n",
    "    for k, v in input_dict.items():\n",
    "        print(f'CAPUT {k} {v}')\n",
    "        caput(k, v)\n",
    "\n",
    "    sleep(2.0)\n",
    "\n",
    "    # get beam sizes from image diagnostic\n",
    "    metadata = input_dict\n",
    "    results = image_diagnostic.measure_beamsize(3, **metadata)\n",
    "    results[\"S_x_mm\"] = np.array(results[\"Sx\"]) * 1e-3\n",
    "    results[\"S_y_mm\"] = np.array(results[\"Sy\"]) * 1e-3\n",
    "\n",
    "    # get other PV's NOTE: Measurements not synchronous with beamsize measurements!\n",
    "    results = results\n",
    "\n",
    "    # add total beam size\n",
    "    results[\"total_size\"] = np.sqrt(np.array(results[\"Sx\"]) ** 2 + np.array(results[\"Sy\"]) ** 2)\n",
    "    # results[\"total_size\"] = np.sqrt(np.abs(np.array(results[\"Sx\"])) * np.array(results[\"Sy\"]))\n",
    "    \n",
    "    # GP model predictions\n",
    "    model_predictions = get_model_predictions(input_dict, generator)\n",
    "    results.update(model_predictions)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define NN prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lume_model.utils import variables_from_yaml\n",
    "from lume_model.torch import LUMEModule, PyTorchModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"lcls_cu_injector_nn_model/\"\n",
    "\n",
    "# # load nn_to_cal transformers\n",
    "# reg = \"low\"  # \"low\" or \"high\"\n",
    "# input_nn_to_cal = torch.load(f\"calibration/input_nn_to_cal_{reg}_reg.pt\")\n",
    "# output_nn_to_cal = torch.load(f\"calibration/output_nn_to_cal_{reg}_reg.pt\")\n",
    "\n",
    "# load sim_to_nn transformers\n",
    "input_sim_to_nn = torch.load(model_path + \"model/input_sim_to_nn.pt\")\n",
    "output_sim_to_nn = torch.load(model_path + \"model/output_sim_to_nn.pt\")\n",
    "\n",
    "# load pv_to_sim transformers\n",
    "input_pv_to_sim = torch.load(model_path + \"model/input_pv_to_sim.pt\")\n",
    "output_pv_to_sim = torch.load(model_path + \"model/output_pv_to_sim.pt\")\n",
    "\n",
    "# load in- and output variable specification\n",
    "input_variables, output_variables = variables_from_yaml(open(model_path + \"model/pv_variables.yml\"))\n",
    "# input_variables, output_variables = variables_from_yaml(open(f\"calibration/pv_variables_{reg}_reg.yml\"))\n",
    "\n",
    "# replace keys in input variables\n",
    "input_variables = {name.replace(\"BACT\", \"BCTRL\"): ele for name, ele in input_variables.items()}\n",
    "\n",
    "# create LUME-model\n",
    "lume_model = PyTorchModel(\n",
    "    model_file=model_path + \"model/model.pt\",\n",
    "    input_variables=input_variables,\n",
    "    output_variables=output_variables,\n",
    "    input_transformers=[input_pv_to_sim, input_sim_to_nn],\n",
    "    output_transformers=[output_sim_to_nn, output_pv_to_sim],\n",
    "    # input_transformers=[input_pv_to_sim, input_sim_to_nn, input_nn_to_cal],\n",
    "    # output_transformers=[output_nn_to_cal, output_sim_to_nn, output_pv_to_sim],\n",
    ")\n",
    "\n",
    "# wrap in LUMEModule\n",
    "lume_module = LUMEModule(\n",
    "    model=lume_model,\n",
    "    feature_order=vocs.variable_names,\n",
    "    output_order=lume_model.outputs[0:2],\n",
    ")\n",
    "\n",
    "# define objective model\n",
    "class ObjectiveModel(torch.nn.Module):\n",
    "    def __init__(self, model: LUMEModule):\n",
    "        super(ObjectiveModel, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    @staticmethod\n",
    "    def function(sigma_x: torch.Tensor, sigma_y: torch.Tensor) -> torch.Tensor:\n",
    "        # using this calculation due to occasional negative values\n",
    "        return torch.sqrt(sigma_x ** 2 + sigma_y ** 2)\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        idx_sigma_x = self.model.output_order.index(\"OTRS:IN20:571:XRMS\")\n",
    "        idx_sigma_y = self.model.output_order.index(\"OTRS:IN20:571:YRMS\")\n",
    "        sigma_x = self.model(x)[..., idx_sigma_x]\n",
    "        sigma_y = self.model(x)[..., idx_sigma_y]\n",
    "        return self.function(sigma_x, sigma_y)\n",
    "\n",
    "objective_model = ObjectiveModel(lume_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restrict ranges based on profile monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocs.variables = {k: input_variables[k].value_range for k in vocs.variable_names}\n",
    "vocs.variables[\"SOLN:IN20:121:BCTRL\"] = [0.467, 0.479]\n",
    "print(vocs.as_yaml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Xopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-17T02:43:02.453721600Z",
     "start_time": "2023-08-17T02:43:02.097709300Z"
    }
   },
   "outputs": [],
   "source": [
    "from xopt import Xopt, VOCS\n",
    "from xopt.evaluator import Evaluator\n",
    "from xopt.numerical_optimizer import LBFGSOptimizer\n",
    "from xopt.generators import ExpectedImprovementGenerator\n",
    "from xopt.generators.bayesian.models.standard import StandardModelConstructor\n",
    "\n",
    "# remember to set use low noise prior to false!!!\n",
    "model_constructor = StandardModelConstructor(\n",
    "    use_low_noise_prior=False,\n",
    "    mean_modules={\"total_size\": objective_model},\n",
    ")\n",
    "generator = ExpectedImprovementGenerator(\n",
    "    vocs=vocs,\n",
    "    model_constructor=model_constructor,\n",
    "    # turbo_controller=\"optimize\"\n",
    ")\n",
    "generator.numerical_optimizer.max_iter = 200\n",
    "evaluator = Evaluator(function=eval_beamsize, function_kwargs={\"generator\": generator})\n",
    "X = Xopt(generator=generator, evaluator=evaluator, vocs=vocs)\n",
    "X.dump_file = run_dir + \"nn_optimization_7d_1.yml\"\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.random_evaluate(3, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    X.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.data.to_csv(run_dir + \"nn_optimization_7d_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.data.plot(y=\"total_size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.data.plot(y=X.vocs.variable_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.data[[\"total_size\" + k for k in [\"\", \"_prior_mean\", \"_posterior_mean\", \"_posterior_sd\"]]].plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_model_in_2d\n",
    "\n",
    "fig, ax = plot_model_in_2d(\n",
    "    X=X,\n",
    "    output_name=\"total_size\",  # should also work for constraints\n",
    "    variable_names=None,  # provides option to view 2D slices at higher dimensions\n",
    "    constrained_acqf=True,  # display the constrained or basic acquisition function\n",
    "    n_grid=50,  # number of grid points per dimension\n",
    "    figsize=(10,8),\n",
    "    show_samples=True,\n",
    "    fading_samples=True,  # older samples are more transparent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_model_in_2d(\n",
    "    X=X,\n",
    "    output_name=\"bb_penalty\",  # should also work for constraints\n",
    "    variable_names=None,  # provides option to view 2D slices at higher dimensions\n",
    "    constrained_acqf=False,  # display the constrained or basic acquisition function\n",
    "    n_grid=50,  # number of grid points per dimension\n",
    "    figsize=(10,8),\n",
    "    show_samples=True,\n",
    "    fading_samples=True,  # older samples are more transparent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.utils.read_files import read_file\n",
    "res = read_file(X.data.iloc[-1][\"save_filename\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(res[\"images\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nnprior]",
   "language": "python",
   "name": "conda-env-nnprior-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
